---
layout: post
title: ! 'Bayesian Statistics: Estimating the General Linear Model'
date: 
type: post
published: false
status: draft
categories:
- How-to
- Research Methodology
tags: []
meta:
  _edit_last: '15848'
  editInHTML: 'off'
author:
  login: Fabian Dablander
  email: dostodabsi@gmail.com
  display_name: Fabian Dablander
  first_name: Fabian
  last_name: Dablander
---
<h2>Introduction</h2>
<p>I want to start off with a very interesting observation from Cohen:</p>
<blockquote><p>If you should say to a mathematical statistician that you have discovered that linear multiple regression analysis and the analysis of variance (and covariance) are identical systems, he would mutter something like, “Of course – general linear model”, and you might have trouble maintaining his attention span. If you should say this to a typical psychologist, you would be met with incredulity, or worse. </p>
<p>-- Cohen(1968, p.426)</p></blockquote>
<p>In many university curricula, regression and analysis of variance are treated separately, most likely because the respective techniques are applied in different areas of research. While ANOVA is common in <em>experimental</em> psychology, where one can systematically vary the independent variable, multiple regression is common in <em>correlational</em> psychology, i.e. survey research (Cronbach, 1957). </p>
<p>This is an example of how - due to mostly <em>historical reasons</em> - we are now a little worse off. A graver and deeper confusion, however, stems from the fact that researchers in psychology who wrote the first textbooks on statistics in the 20th century ignored the history and controversies in statistics, mashing together conflicting ideas from R.A. Fisher and Neyman-Pearson (Gigerenzer, 1993). This resulted in a hybrid approach to statistical testing commonly referred to as NHST (Null Hypothesis Significance Testing). Its p-value and confidence intervals are misunderstood by many students (Oakes, 1986), as well as their teachers (Haller & Krauss, 2002; Hoekstra, Morey, Rouder, & Wagenmakers, 2014).</p>
<p>In many cases, researchers want to eat the cake without paying for it, i.e. they mistake the probability of the data given the hypothesis $P(Data|Hypothesis)$ for the probability of the hypothesis given the data $P(Hypothesis|Data)$. The latter is what we <b>really want</b>, but what the p-value <b>cannot</b> provide. For this we need Bayesian statistics.</p>
<p>In this blogpost, I want to bridge the statistically artificial gap between <em>experimental</em> and <em>correlational</em> psychology by introducing the General Linear Model. Instead of using the defective NHST approach, I will estimate these models using Bayesian techniques. Using the simulated creativity data from the previous blogpost, I add the continuous predictor intelligence to show how easily the approach generalizes. If you have not done so yet, please read the <a href="http://blog.efpsa.org/2014/11/17/bayesian-statistics-what-is-it-and-why-do-we-need-it-2/">previous</a> blog post as a prerequisite to the current one.</p>
<p>Note that this blogpost discusses several statistical concepts which - if entirely new to you - might make the following difficult to understand. Although there is a common theme, I touch on many things only cursory. It is my hope that you will be sufficiently impressed by the Bayesian approach so as to read some of the suggested papers. Most of them offer an in depth treatment of a technical problem, while at the same time being clear and exceptionally well-written. Additionally, we will use <a href="http://www.r-project.org/" target="_blank">R</a> and <a href="http://mcmc-jags.sourceforge.net/" target="_blank">JAGS</a>. In order to run the code, you need to have both installed.</p>
<h2>General Linear Model</h2>
<p>General linear models are of the form:<br />
<center><br />
$response = deterministic \ part + stochastic \ part$<br />
</center></p>
<p>The stochastic part is described by a statistical distribution, while the deterministic part is specified by the linear predictor (Kery, 2010 p.57).</p>
<h3>Statistical Distributions</h3>
<p>While there are many statistical distributions, the ones below occur most frequently. They describe the random variation in the response. For a nice, interactive playground with plenty of distributions see <a href="http://spark.rstudio.com/uafsnap/RV_distributionsV4/" target="_blank">this</a>.</p>
<p><center><br />
<a href="http://blog.efpsa.org/wp-content/uploads/2015/01/distributions.png"><img src="{{ site.baseurl }}/assets/distributions-300x300.png" alt="distributions" width="300" height="300" class="alignnone size-medium wp-image-3161" /></a><br />
</center></p>
<p>In the General Linear Model the errors are assumed to be normally distributed</p>
<p><center><br />
$\varepsilon \sim \mathcal{N} (0, \sigma^2)$<br />
</center></p>
<h3>Linear Predictor</h3>
<p>The linear predictor is of the form:</p>
<p><center><br />
$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + … + \beta_n x_{in} + \epsilon_i$<br />
</center></p>
<p>where $y_i$ are the outcomes, $x_n$ the predictors, $\beta_n$ the parameters we want to estimate, and $\varepsilon_i$ is the error. For a more visual demonstration of this in action, checkout <a href="http://glimmer.rstudio.com/winston/heightweight/" target="_blank">this</a> RShiny app.</p>
<h2>Why do we need JAGS?</h2>
<blockquote><p>An ounce of algebra is worth a ton of verbal argument.<br />
- J.B.S. Haldane</p></blockquote>
<p>Bayesian statistics is fundamentally superior to Frequentist statistics, but since it requires complicated computations was rendered impractical for most of the 20<sup>th</sup> century (McGrany, 2010). The heart of Bayesian statistics is Bayes' Rule which was independently discovered by Reverend Thomas Bayes and the great Pierre-Simon Laplace. Laplace extended and applied it and is considered the first Bayesian. Bayes' Rule itself is not the least bit controversial, since it follows straightforwardly from the axioms of probability.</p>
<p>Recall Bayes’ Rule over some parameter $\theta$ and data $D$:</p>
<p><center><br />
$p(\theta|D) = \frac{p(\theta) p(D|\theta)}{p(D)}$<br />
</center></p>
<p>The computationally hard part is $p(D)$ which is the probability of the data given all possible values of $\theta$. It’s</p>
<p><center><br />
$p(D) = \int p(\theta) p(D|\theta) \mathrm{d\theta}$<br />
</center></p>
<p>for continuous distributions. This term is sometimes called <strong>marginal likelihood</strong>, since $\theta$ gets “integrated out". In other words, we compute the likelihood of the data for each value of $\theta$ and weight it by the prior probability of $\theta$. We sum over all those results, yielding $p(D)$. We need $p(D)$ to compute the posterior distribution $p(\theta|D)$ and finally eat the cake. However, most of the times solving this integral analytically - that is, “by hand” - is impossible. There are sidesteps that capitalize on increasing computational power. Instead of computing $p(D)$, we sample from the posterior distribution directly. This yields:</p>
<p><center><br />
$p(\theta|D) \sim p(\theta) p(D|\theta)$<br />
</center></p>
<p>This is accomplished using <a href="https://stats.stackexchange.com/questions/165/how-would-you-explain-markov-chain-monte-carlo-mcmc-to-a-layperson" target="_blank">Markov Chain Monte Carlo</a> methods. One such technique is called Gibbs sampling, which is what JAGS does. JAGS is short for <b>J</b>ust <b>A</b>nother <b>G</b>ibbs <b>S</b>ampler.</p>
<p>Summing up, in order to compute the posterior distribution using Bayes’ Rule we need to compute the marginal likelihood. Since that’s impossible for most interesting cases, we use a Gibbs Sampler called JAGS to sidestep this computation and sample from the posterior distribution directly. Note that the marginal likelihood is essential for the Bayes factor. JAGS can only estimate posterior distributions of parameters, not compute Bayes factors directly<a href="#0">[0]</a>. But more on that later.</p>
<h3>Preparation</h3>
<p>Before we start, let’s prepare the dataset from the previous blogpost:</p>
<pre class="theme:familiar toolbar:2 lang:r decode:true ">
set.seed(666)

mu1 &lt;- 103
mu2 &lt;- 98
sigma &lt;- 15

y1 &lt;- rnorm(100, mu1, sigma)
y2 &lt;- rnorm(100, mu2, sigma)
creativity &lt;- c(y2, y1)
hat &lt;- rep(c(0, 1), each = 100)

df &lt;- data.frame(creativity, hat)</pre>
<h2>The t-test: a simple General Linear Model</h2>
<p>We want to see if there is a difference in creativity between people who wear fancy hats compared to those who don’t. In other words, we want to predict creativity via the variable $hat$, which is dummy coded to $0$ (no hat) and $1$ (hat). Using the formula of General Linear Models above, this yields</p>
<p><center><br />
$creativity_i = \beta_0 + \beta_1  hat_i + \varepsilon_i$<br />
</center></p>
<pre class="lang:r theme:familiar toolbar:2 decode:true " title="setup data">

library('rjags') # used to inferface with JAGS
library('BayesFactor') # our old and trusted friend

N &lt;- nrow(df)
head(df, 4)

## creativity hat
## 1 83.79 0
## 2 70.75 0
## 3 127.78 0
## 4 110.25 0</pre>
<p>In Frequentist statistics, we want to pick values for $\beta_0$ and $\beta_1$ such that the difference between the actual values and the predicted values is minimized<a href="#1">[1]</a>:</p>
<p><center><br />
$83.78 = \beta_0  1 + \beta_1  0 + \varepsilon_i$<br />
$70.75 = \beta_0  1 + \beta_1  0 + \varepsilon_i$</p>
<p>$\ldots$</p>
<p>$105.66 = \beta_0  1 + \beta_1  1 + \varepsilon_i$<br />
$103.64 = \beta_0  1 + \beta_1  1 + \varepsilon_i$<br />
</center></p>
<p>or, put more concisely, we want to minimize S:</p>
<p><center><br />
$S = \sum_{i=1}^{N} \varepsilon_i{^2} = \sum_{i=1}^{N} (y_i - \beta_0 + \beta_1 x_{i1})$<br />
</center></p>
<p>We can easily see that $\beta_0$ must correspond to the mean value in creativity of people who don’t wear hats, while $\beta_1$ must be the amount of creativity that people wearing hats additionally have. Recall the outcome of the previous “BayesFactor” analysis:</p>
<pre class="lang:r decode:true theme:familiar toolbar:2" title="bayes factor">lmBF(creativity ~ hat, data = df)

## Bayes factor analysis
## --------------
## [1] hat : 1.977 ±0%
##
## Against denominator:
## Intercept only
## ---
## Bayes factor type: BFlinearModel, JZS</pre>
<p>We get a Bayes factor of $1.98$. This means that the model which assumes a difference between people who wear hats compared to people who wear no hat (i.e. $H_1$) is $1.98$ times more likely then the "Intercept only" model, which assumes no difference (i.e. $H_0$) - given our prior beliefs and the collected data.</p>
<h2>Two types of priors</h2>
<h3>Prior on parameters</h3>
<p>In the output above, it says <strong>Bayes factor type: BFlinearModel, JZS</strong>. BFlinearModel should be clear by now, but what does JZS mean? As you might have guessed, it specifies a certain type of prior distribution: the JZS-prior describes a Cauchy distribution on effect size $\delta$ and the <a href="https://en.wikipedia.org/wiki/Jeffreys_prior" target="_blank">Jeffrey's prior</a> on the variance $\sigma^2$ (Rouder et al., 2009):</p>
<p><center><br />
$\delta \sim Cauchy(0, 1)$<br />
$p(\sigma^2) = \frac{1}{\sigma^2}$<br />
</center></p>
<p>The Cauchy distribution is a t-distribution with just one degree of freedom. Simply put, it's a normal distribution with fat tails:</p>
<p><center><br />
<a href="http://blog.efpsa.org/wp-content/uploads/2015/01/cauchy_gauss.png"><img src="{{ site.baseurl }}/assets/cauchy_gauss-300x300.png" alt="cauchy_gauss" width="300" height="300" class="alignnone size-medium wp-image-3307" /></a><br />
</center></p>
<p>The advantage of specifying a distribution over the effect size instead of the mean is that it can be used across many experiments. Also, limiting the prior for $\sigma$ only to positive values makes sense, since there is no negative standard deviation. These prior distributions are so called <b>objective priors</b> and are motivated in Rouder et al. (2009), which is worth the read!</p>
<h3>The Bayes factor and prior on models</h3>
<p>The Bayes factor was introduced by Harold Jeffreys in the 1960s and is the Bayesian solution for model comparison. Note that model comparison is a general term which also encompasses testing (null) hypotheses. For an excellent overview, see Vandekerckhove, Matzke, and Wagenmakers, 2014. Model comparison using Bayes factors amounts to</p>
<p><center><br />
$\text{posterior odds} = \text{Bayes factor} × \text{prior odds}$<br />
</center></p>
<p>which, put more formally, describes how strongly some model $M_1$ is favoured over some other model $M_0$:</p>
<p><center><br />
$\text{posterior odds} = \frac{p(D|M_1)}{p(D|M_0)} \frac{p(M_1)}{p(M_0)}$<br />
</center></p>
<p>It is important to note that the Bayes Factor describes only the first term. Therefore, we have <b>two priors</b> (Lee & Wagenmakers, 2013, p.111). The first one concerns the prior odds. It specifies the plausibility we assign to different models or hypotheses. In our creativity example, if someone assigns more belief to the model that specifies no difference by a ratio of 1:10, we get the following:</p>
<p><center><br />
$\text{posterior odds} = 1.98  \frac{1}{10}$<br />
</center></p>
<p>which means that the model that assumes no influence of hats on creativity is favoured by a factor of $\frac{1}{.198} \sim 5$. This is the first type of prior, i.e. one that assigns credibility to whole models. Note that the Bayes factor is not influenced by this kind of prior.</p>
<p>An important point concerning the prior odds is that <em>extraordinary claims require extraordinary evidence</em>. Recall the <b>battle of BEM</b>, i.e. the publication of an article claiming that people can see into the future (Bem, 2011). Now, how strong is your belief in that possibility? I say it's fairly low, say $\frac{1}{1000000}$. Therefore, even when we get a reasonably high Bayes factor, say 15, in favour of BEM's claim, weighted by our prior odds we should still not be convinced that people can predict the future:</p>
<p><center><br />
$\text{posterior odds} = \frac{15}{1} \frac{1}{1000000} = 0.000015$<br />
</center></p>
<p>The model that assumes that people cannot predict the future is $\frac{1}{0.000015} = 66666 \frac{2}{3}$ times more likely than the model that assumes that people can predict the future.</p>
<p>Another important point is that one should not assign zero probability to a certain model or hypotheses. Since however strong the evidence collected is, it can never change your mind. This is sometimes called Cromwell's Rule:<br />
<blockquote>I beseech you, in the bowel's of Christ, think it possible that you might be mistaken.</p></blockquote>
<p>The second type of prior is a little sneakier. Recall that</p>
<p><center><br />
$p(D|M) = \int p(D|\theta, M)  p(\theta|M) \mathrm{d}\theta$<br />
</center></p>
<p>This is the same $p(D)$ as in the first section, but specific for some model $M$. We compute the likelihood of the data given the model for each value of $\theta$ and weight it by the prior probability of $\theta$. We sum over all those results, yielding $p(D|M)$. This means that the prior distribution over the parameters might greatly influence the Bayes factor.</p>
<p>Let me give a quick example, adjusted from Lee & Wagenmakers (2013, pp. 101). Assume a simple model $M_x$ with one parameter $\mathcal{E}$ which can take three values. We specify the prior distribution over these values, such that $p(\mathcal{E}_1) = .8$, $p(\mathcal{E}_2) = .1$ and $p(\mathcal{E}_3) = .1$. The likelihoods for the parameter values are $p(D|\mathcal{E}_1) = .001$, $p(D|\mathcal{E}_1) = .002$ and $p(D|\mathcal{E}_1) = .003$. We can compute the marginal likelihood by averaging the respective likelihoods weighted with their priors:</p>
<p><center><br />
$p(D|M_x) = p(\mathcal{E}_1) p(D|\mathcal{E}_1) + p(\mathcal{E}_2) p(D|\mathcal{E}_2) + p(\mathcal{E}_3) p(D|\mathcal{E}_3)$</p>
<p>$p(D|M_x) = .8 * .001 + .1 * .002 + .1 * .003 = .0013$<br />
</center></p>
<p>If we simply reverse the prior, we get</p>
<p><center><br />
$p(D|M_{x'}) = .1 * .001 + .1 * .002 + .8 * .003 = .0027$<br />
</center></p>
<p>yielding a Bayes factor in support of $M_x'$ of</p>
<p><center><br />
$BF_{x'x} = \frac{p(D|M_{x'})}{p(D|M_x)} = \frac{.0027}{.0013} \sim 2.08$<br />
</center></p>
<p>Although one has to be thoughtful when specifying priors, they are the key to automatic parsimony (Vandekerckhove et al., 2014). Since overly complex models predict a range of phenomena, they spread out the prior probability of their parameters which results in a smaller marginal likelihood. For example, assume a model $M_z$ with one parameter that can take on 5 values $\mu_1, \mu_2, ..., \mu_5$ with equal prior probability $\frac{1}{5}$ and likelihoods $p(D|\mu_{1-4}) = .001$ and $p(D|\mu_5) = .006$, yielding</p>
<p><center><br />
$p(D|M_z) = 4 * (.2 + .001) + .2 * .006 = .002$<br />
</center></p>
<p>This model is more complex, since it can account for more possible datasets. However, it has to spread out its prior probability. The prior probabilities of the parameter values act as weight in computing the marginal likelihood, such that it automatically decreases in complex models: $BF_{x'z} = 1.35$.</p>
<p>For a nice treatment of how priors effect the conclusion, see <a href="http://www.nicebread.de/interactive-exploration-of-a-priors-impact/" target="_blank">this</a> blogpost about <b>sensitivity analysis</b>, showing that for reasonable priors, they do not substantially impact the conclusions drawn.</p>
<p>Note that in Bayesian statistics the null hypothesis has no special status. We can, in contrast to Frequentist statistics, obtain evidence <b>in favour</b> of $H_0$ (Rouder et al., 2009). Furthermore, we also explicitly pay for our lunch, that is look at the probability of the data under $H_1$ (Rouder et al., 2014).</p>
<p>As discussed, JAGS is a tool for Bayesian <b>estimation of parameters</b>. It avoids computing the marginal likelihood, which is key to the Bayes factor. Therefore we only estimate parameters in the following sections, focusing on using JAGS for a t-test and multiple regression.</p>
<h2>Parameter Estimation</h2>
<p>If you run a statistical model using frequentist techniques, say a t-test:</p>
<pre class="lang:r decode:true toolbar:2 theme:familiar" title="t-test lm">
lm(creativity ~ hat, df)

## Call:
## lm(formula = creativity ~ hat, data = df)

## Coefficients:
## (Intercept)          hat  
##    96.608           5.392  
</pre>
<p>you don't get a p-value. This is because R is a respectable statistical tool, which knows that the p-value is secondary. The first step is to estimate the parameters. The second step might be to test their significance, broadly speaking. In this blogpost we will focus on the first step.</p>
<h2>t-test: Using JAGS</h2>
<p>In JAGS, we specify distributions over parameters with the $\sim$ symbol. For example,<br />
$dnorm(0, .0001)$ specifies a normal distribution with mean 0 and precision .0001, where<br />
<center><br />
$\text{precision} = \frac{1}{\text{variance}}$<br />
</center></p>
<p>For the standard deviation $\sigma$ we also choose an uninformative prior: a uniform distribution ranging from 0 to 1000. Note that in JAGS the variance is always specified as precision. We then loop over each row in our dataframe, specifying $\mu$ via the linear predictor, and drawing samples from</p>
<p><center><br />
$y_i \sim \mathcal{N} (\beta_0 + \beta_1 x_i, \sigma^2) \sim \mathcal{N} (\mu, \sigma^2)$<br />
</center></p>
<pre class="lang:r decode:true toolbar:2 theme:familiar" title="t-test in jags">model_string &lt;- '
model {
    # specifying priors over parameter
    b0 ~ dnorm(0, 0.001)
    b1 ~ dnorm(0, 0.001)
    sigma ~ dunif(0, 1000)
    tau &lt;- 1 / sigma * sigma
    
    # for each data point
    for (i in 1:N) {
        mu[i] &lt;- b0 + b1 * x[i]
        y[i] ~ dnorm(mu[i], tau)
    }
}'</pre>
<p>We have to specify the variables used by JAGS. In this case, the predictor $x$(no hat / hat), the outcome $y$ (creativity) and the number of data points $N$. We then connect to JAGS with the <b>jags.model</b> function, which takes the model string and a list of variables as arguments. After that, we draw samples from the posterior distribution of our parameters $\beta_0$ and $\beta_1$ using the <b>coda.samples</b> function:</p>
<pre class="lang:r decode:true toolbar:2 theme:familiar" title="t-test model setup"># important for model specification
data &lt;- list('x' = df$hat, 'y' = df$creativity, 'N' = N)

model &lt;- jags.model(textConnection(model_string),
                    data = data, n.chains = 2, quiet = TRUE)

params &lt;- c('b0', 'b1') # should be estimated

# sample from the posterior distribution
samples &lt;- coda.samples(model, n.iter = 10000,
                        variable.names = params)</pre>
<p>$\beta_0$, as you recall from our previous discussion, is the expected value for creativity given no hat, i.e. when the predictor is dummy-coded to 0. $\beta_1$ is the amount that gets added to this expected value when there is a hat. You can get a nice summary via:</p>
<pre class="lang:r decode:true theme:familiar toolbar:2" title="t-test output">summary(samples)

##
## Iterations = 1001:11000
## Thinning interval = 1
## Number of chains = 2
## Sample size per chain = 10000
##
## 1. Empirical mean and standard deviation for each variable,
## plus standard error of the mean:
##
##     Mean   SD    Naive SE   Time-series SE
## b0 96.61  0.101   0.00101       0.00168
## b1 5.39   0.145   0.00145       0.00250
##
## 2. Quantiles for each variable:
##
##      2.5%   25%   50%   75%   97.5%
## b0 96.406  96.54  96.61  96.68  96.80
## b1  5.110   5.30   5.39   5.49   5.68
</pre>
<p>Plotting the posterior distribution for $\beta_0$ and $\beta_1$ we get</p>
<pre class="lang:default decode:true theme:familiar toolbar:2" title="plot t-test posterior">library('ggmcmc') # for nice plots

ggs_density(ggs(samples))</pre>
<p><center><br />
<a href="http://blog.efpsa.org/wp-content/uploads/2015/01/ttest.png"><img src="{{ site.baseurl }}/assets/ttest-300x300.png" alt="ttest" width="300" height="300" class="alignnone size-medium wp-image-3167" /></a><br />
</center></p>
<h2>Multiple Regression</h2>
<p>Linear regression is a t-test with a continuous instead of a dichotomous predictor. When we have more than one continuous predictor variable, we speak of multiple regression. If the predictor variables are categorical, we call the model Analysis of Variance. If we mix both continuous and categorical predictors, we use the term Analysis of Covariance. However, if we look at the code, it basically amounts to the same thing<a href="#2">[2]</a>.</p>
<p>We simulate the intelligence data until it correlates with creativity $\sim$ $0.3$, and add it to our creativity example:</p>
<pre class="lang:r decode:true theme:familiar toolbar:2 " title="add intelligence covariate">done &lt;- function(x, y, r) { abs(cor(x, y) - r) &lt; .05 }
int &lt;- rnorm(200, mean=100, sd=15)

# creativity and intelligence should correlate ~ .3
while (!done(int, creativity, .3))
    int &lt;- rnorm(200, mean=100, sd=15)
    
df$intelligence &lt;- int

head(df, 4)
##   creativity hat intelligence
##       83.79   0   109.34
##       70.75   0   90.61
##      127.78   0   100.39
##      110.25   0   103.66
</pre>
<p>The linear predictor changes only marginally:</p>
<p><center><br />
$creativity_i = \beta_0 + \beta_1  hat_i + \beta_2  intelligence_i + \varepsilon_i$<br />
</center></p>
<p>The inference we draw now is</p>
<p><center><br />
$y_i \sim \mathcal{N} (\beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}, \sigma^2)$<br />
$y_i \sim \mathcal{N} (\mu, \sigma^2)$<br />
</center></p>
<pre class="lang:r decode:true theme:familiar toolbar:2" title="multiple regression in jags">model_string &lt;- '
model {
    b0 ~ dnorm(0, 0.001)
    b1 ~ dnorm(0, 0.001)
    b2 ~ dnorm(0, 0.001) # added one more prior
    sigma ~ dunif(0, 1000)
    tau &lt;- 1 / sigma * sigma
    
    # for each data point
    for (i in 1:N) {
        # only difference is in the linear predictor!
        mu[i] &lt;- b0 + b1 * x1[i] + b2 * x2[i]
        y[i] ~ dnorm(mu[i], tau)
    }
}'</pre>
<p>Multiple regression consists of more parameters, and hence we must specify more prior distributions, as well as an extended linear predictor. Everything else is equal to the case of a t-test.</p>
<pre class="lang:r decode:true toolbar:2 theme:familiar" title="regression model setup"># important for model specification
data &lt;- list('y' = df$creativity, 'x1' = df$hat,
             'x2' = df$intelligence, 'N' = N)
             
model &lt;- jags.model(textConnection(model_string),
                    data = data, quiet = TRUE)
                    
params &lt;- c('b0', 'b1', 'b2')
samples &lt;- coda.samples(model, n.iter = 10000,
                        variable.names = params)</pre>
<p>What the last two examples demonstrate is that there is only superficial difference between a multiple regression and a t-test with respect to their model specifications. However, bear in mind that regression makes a lot more assumptions about the data. For the use and misuse of Analysis of Covariance, see Miller and Chapman, 2001.</p>
<pre class="lang:r decode:true theme:familiar toolbar:2" title="regression parameter estimation">summary(samples)

##
## Iterations = 1001:11000
## Thinning interval = 1
## Number of chains = 1
## Sample size per chain = 10000
##
## 1. Empirical mean and standard deviation for each variable,
## plus standard error of the mean:
##
##      Mean      SD  Naive SE   Time-series SE
## b0 67.626 0.50080 5.01e-03          0.045599
## b1  5.180 0.14207 1.42e-03          0.002486
## b2  0.289 0.00487 4.87e-05          0.000462
##
## 2. Quantiles for each variable:
##
##     2.5%   25%    50%    75%   97.5%
## b0 66.66 67.275 67.640 67.974 68.569
## b1  4.90  5.084  5.182  5.273  5.456
## b2  0.28  0.286  0.289  0.293  0.299
</pre>
<h2>Can I haz Hypothesis Testing?</h2>
<p>To reiterate: we only estimated parameters, we did not test hypotheses. In most of psychology, however, testing hypotheses is the non plus ultra. For this, we need the Bayes factor.</p>
<p>As previously stated, the Bayes factor is difficult to compute. However, there exists an incredible elegant method to compute it that applies for nested models only (this is a restriction that the p-value also has!). This method is called the <b>Savage-Dickey densitio ratio</b>. I feel that this blogpost has gone so long you must be fighting off zombies by now. Therefore I will just recommend you Wagenmakers et al. (2010) for an excellent introduction.</p>
<h2>Conclusion</h2>
<p>We have seen that the t-test, ANOVA and multiple regression are just variants of the General Linear Model, which consists of a simple deterministic part and a stochastic part. Using R and JAGS we implemented a t-test and ANCOVA and estimated the parameters in a Bayesian fashion.</p>
<p>If you are intrigued by what I have shown you thus far, you might want to check out <a href="https://github.com/dostodabsi/jags_models" target="_blank">this</a> Github repository, where I have implemented some more models (t-test with unequal variance, poisson regression, random effects ANOVA, hierarchical linear models etc.). Also, there is no substitute for a good book. I recommend the following, which you might want to cross-read:</p>
<ul>
<li><a href="http://www.amazon.com/Bayesian-Cognitive-Modeling-Practical-Course/dp/1107603579/ref=sr_1_1?ie=UTF8&qid=1416421852&sr=8-1&keywords=cognitive+modeling+practical+course">Bayesian Cognitive Modeling</a> (hands on stuff - must have!)</li>
<li><a href="http://www.amazon.com/Introduction-WinBUGS-Ecologists-Bayesian-regression/dp/0123786053/ref=sr_1_1?ie=UTF8&qid=1416421796&sr=8-1&keywords=winbugs+for+ecologist">WinBUGS for Ecologists</a> (first few chapters on GLMs)</li>
<li><a href="http://www.amazon.com/Doing-Bayesian-Data-Analysis-Second/dp/0124058884/ref=sr_1_1?ie=UTF8&qid=1416421825&sr=8-1&keywords=doing+bayesian+data+analysis">Doing Bayesian Data Analysis</a> (provides the most extensive treatment)</li>
</ul>
<h3>Suggested Readings</h3>
<ul>
<li>
<a href="https://www.mpib-berlin.mpg.de/volltexte/institut/dok/full/gg/ggstehfda/ggstehfda.html" target="_blank">The superego, the ego, and the id in statistical reasoning</a></li>
<li>
<a href="http://www.metheval.uni-jena.de/lehre/0405-ws/evaluationuebung/haller.pdf" target="_blank">Misinterpretations of significance: a problem students share with their teachers</a></li>
<li>
<a href="http://www.ejwagenmakers.com/2008/BayesFreqBook.pdf" target="_blank">Bayesian versus frequentist inference</a></li>
<li>
<a href="http://ejwagenmakers.com/2010/WagenmakersEtAlCogPsy2010.pdf" target="_blank">Bayesian hypothesis testing for psychologists: a tutorial on the savage–dickey method</a></li>
<li>
<a href="http://pcl.missouri.edu/jeff/sites/pcl.missouri.edu.jeff/files/Rouder.etal_.pbr_.2009.pdf" target="_blank">Bayesian t tests for accepting and rejecting the null hypothesis</a></li>
</ul>
<h3>Footnotes</h3>
<div id="#0">
<a name="0">[0]</a>This is not strictly true. One can use techniques like transdimensional MCMC to compute the Bayes factor directly. For the purpose of this blogpost and to keep it simple, however, we will ignore these techniques.
</div>
<p></p>
<div id="#1">
<a name="1">[1]</a> This is called Ordinary Least Square Estimation. For GLMs, it is equivalent to Maximum Likelihood Estimation.
</div>
<p></p>
<div id="#2">
<a name="2">[2]</a> The t-test actually uses the t-distribution, but when the degrees of freedom > 30, the t-distribution approximates a normal distribution. In fact, since the t-distribution has heavier tails (the smaller the degrees of freedom, the heavier the tails), it can be used to make one's estimation more robust. See for example <a href="http://www.sumsar.net/blog/2013/08/robust-bayesian-estimation-of-correlation/" target="_blank">this</a>.
</div>
<h2>References</h2>
<p>Bem, D. J. (2011). Feeling the future: experimental evidence for anomalous<br />
retroactive influences on cognition and affect. <em>Journal of personality<br />
and social psychology, 100</em> (3), 407.</p>
<p>Cohen, J. (1968). Multiple regression as a general data-analytic system. <em>Psy-<br />
chological Bulletin, 70</em> (6p1), 426.</p>
<p>Cronbach, L. J. (1957). The two disciplines of scientific psychology. <em>American<br />
psychologist, 12</em> (11), 671.</p>
<p>Gigerenzer, G. (1993). The superego, the ego, and the id in statistical reason-<br />
ing. <em>A handbook for data analysis in the behavioral sciences: Method-<br />
ological issues</em>, 311–339.</p>
<p>Haller, H. & Krauss, S. (2002). Misinterpretations of significance: a problem<br />
students share with their teachers. <em>Methods of Psychological Research,<br />
7 </em>(1), 1–20.</p>
<p>Hoekstra, R., Morey, R. D., Rouder, J. N., & Wagenmakers, E.-J. (2014).<br />
Robust misinterpretation of confidence intervals. <em>Psychonomic bulletin<br />
& review, 21</em> (5), 1–8.</p>
<p>Kéry, M. (2010). Introduction to winbugs for ecologists: bayesian approach to<br />
regression, anova, mixed models and related analyses. Academic Press.<br />
Kruschke, J. (2010). <em>Doing bayesian data analysis: a tutorial introduction<br />
with R</em>. Academic Press.</p>
<p>Lee, M. & Wagenmakers, E.-J. (2013). <em>Bayesian cognitive modeling: a prac-<br />
tical course</em>. Cambridge University Press.</p>
<p>McGrayne, S. B. (2011). <em>The theory that would not die: how bayes’ rule<br />
cracked the enigma code, hunted down russian submarines, & emerged<br />
triumphant from two centuries of controversy</em>. Yale University Press.</p>
<p>Miller, G. A. & Chapman, J. P. (2001). Misunderstanding analysis of covari-<br />
ance. <em>Journal of abnormal psychology, 110</em> (1), 40.</p>
<p>Morey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers,<br />
E.-J. (2014). The fallacy of placing confidence in confidence intervals.<br />
<em>submitted</em>.</p>
<p>Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., & Iverson, G. (2009). Bayesian t tests for accepting and rejecting the null hypothesis. Psychonomic bulletin & review, 16(2), 225-237.</p>
<p>Oakes, M. W. (1986). <em>Statistical inference: a commentary for the social and<br />
behavioural sciences</em>. Wiley New York.</p>
<p>Rouder, J. N., Morey, R. D., Verhagen, J., Province, J. M., Wagenmakers,<br />
E.-J., & Rouder, J. (2014). The p<. 05 rule and the hidden costs of the<br />
free lunch in inference. <em>Manuscript under review</em>.</p>
<p>Rouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., & Iverson, G. (2009).<br />
Bayesian t-tests for accepting and rejecting the null hypothesis. <em>Psy-<br />
chonomic bulletin & review, 16</em> (2), 225–237.</p>
<p>Vandekerckhove, J., Matzke, D., & Wagenmakers, E.-J. (2014). Model com-<br />
parison and the principle of parsimony. <em>Oxford Handbook of Computa-<br />
tional and Mathematical Psychology</em>. Oxford University Press, Oxford.</p>
<p>Wagenmakers, E.-J., Lee, M., Lodewyckx, T., & Iverson, G. J. (2008). Bayesian<br />
versus frequentist inference. In <em>Bayesian evaluation of informative hy-<br />
potheses</em> (pp. 181–207). Springer.</p>
<p>Wagenmakers, E.-J., Lodewyckx, T., Kuriyal, H., & Grasman, R. (2010).<br />
Bayesian hypothesis testing for psychologists: a tutorial on the savage–<br />
dickey method. <em>Cognitive psychology, 60</em> (3), 158–189.</p>
