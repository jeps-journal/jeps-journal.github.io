---
layout: post
title: Why meta-analysis? A guide through basic steps and common biases
date: 2013-07-15 21:44:15.000000000 +02:00
type: post
published: true
status: publish
categories:
- Literature research
- Scientific writing
tags:
- academic writing
- date
- making sense
- meta-analysis
- sources
- writing a paper
meta:
  _edit_last: '95'
author:
  login: LuisTojo
  email: luis.efpsa@gmail.com
  display_name: Luís Miguel Tojo
  first_name: Luís Miguel
  last_name: Tojo
---
<p><b><i><a href="http://blog.efpsa.org/wp-content/uploads/2013/07/file00058776778.jpg"><img class="size-medium wp-image-1913 alignleft" style="margin: 0px 20px 20px 0px;" alt="OLYMPUS DIGITAL CAMERA" src="{{ site.baseurl }}/assets/file00058776778-300x225.jpg" width="300" height="225" /></a></i></b></p>
<p><b><i> Meta:</i></b><i> meta- combining form. </i>From Greek meta ‘with, across, or after.’  Pertaining to a level above or beyond.</p>
<p><b><i> Analysis</i></b>: <i>analysis |əˈnaləsis| noun. </i>From Greek analuō 'I unravel,  investigate'. Detailed examination of the elements or structure of  something,<b></b><br />
<b> </b><br />
Often times, researchers and students find themselves going through a  dense amount of papers on a certain topic only to find results that don't  really seem to point towards a coherent or homogenous conclusion. Does this treatment work?<br />
<!--more-->Well, research A shows yes, but research B and C are quite similar to research A and show the exact opposite results. Which one should I believe in after all? Is the treatment under focus effective or not? This is where meta-analyses can come as a key element. Huque (1988) defined a meta-analysis as "a statistical analysis that combines or integrates the results of several independent clinical trials considered by the analyst to be ‘combinable'.” Nonetheless, there has still been large debate surrounding its proper classification and even some overlap with the definition of "systematic reviews"; however, a systematic review is considered a broader term that characterizes any type of review, and within which "meta-analyses" are included (Egger &amp; Davey Smith, 1997).</p>
<p><b>Okay, great, but why should I use meta-analysis?</b></p>
<p>There are many reasons why a meta-analysis can be a very useful tool in research, particularly when aiming to obtain a more valid methodological approach to a particular hypothesis under which a lot of research has been conducted and you wish to obtain a clear and unidirecional conclusion. These are some reasons that make a meta-analysis such a strong methodological approach (Eggy &amp; Davey Smith, 1997):</p>
<ul>
<li>Single studies are often not reliable enough to detect significant differences between two treatments. Type I and type II errors are the risks that every researcher tries to avoid at all costs, but these are nonetheless very common, particularly in clinical trials (Freiman, Chalmers, Smith, &amp; Kuebler, 1992);</li>
<li>Generalizing the results from a meta-analysis makes more sense than from single studies, considering it integrates different sets of populations into the analysis and, thus, accounts for different variations between different groups which will most likely respond distinctively;</li>
<li>Due to its statistically reliable approach, it makes the review process of the subject at hand much more transparent and less subject to relativity and subjective appraisal from reviewers who conduct narrative reviews.</li>
</ul>
<p><b>You've convinced me, what do I need to do to carry my own meta-analysis?</b></p>
<p>Much like conducting any other type of research, at a very basic level, conducting a meta-analysis runs through three simple steps: (1) formulation of your research question; (2) collection and analysis of the data; (3) reporting the outcomes (Egger, Smith, &amp; Phillips, 1997).</p>
<p>As part of a series around the procedures in conducting meta-analyses, Egger, Smith, &amp; Philips (1997) propose a few essential steps you should be sure to take in order to reach a successful outcome in your future meta-analysis.</p>
<p><b>Step 1:</b> <i>Define eligibility criteria for the data to be included.</i> The criteria will define how compatible the articles to be selected should be between them to make sure we are assessing a common and reliable outcome. This will be based on: (1) quality of trials, (2) type of trials, (3) patients, (4) outcomes, and (5) lengths of follow-up. However, making a sound assessment of the first factor, the quality of trials, will heavily depend on the researcher's judgement, and thus a sensitivity analysis should be performed <i>a posteriori.</i></p>
<p><b></b><b>Step 2:</b> <i>Delineate a strategy for identifying the relevant studies</i>. It is important, in combination with the eligibility criteria, to outline what will be the best strategy, according to your research question and the relevance of your study, to select the studies to be included in your analysis. Particularly, you should consider the inclusion of unpublished studies due to bias effects that are often associated with published studies as will be described further ahead.</p>
<p><b>Step 3:</b> <i>Create a standardised form for data collection</i>. When extracting the data, the most reliable method would be to use two independent observers. For this step, there are scales that have been designed to assess the quality of, for example, clinical trials (for a review, see Moher, Jadad, Nichol, Penman, Tugwell, &amp; Walsh, 1995). Further, the individuals who will be responsible for rating should be blinded to all factors that could influence their assessment, namely: (1) authors and their institutions, (2) names of journals, (3) sources of funding, and (4) acknowledgements.</p>
<p><b>Step 4:</b> <i>Standardise individual results for comparison between studies</i>. After you've collected the data, in order to compare the results you will need to standardize them to something homogenous. For continuous outcomes, one should extract the mean difference between treatment and control group and present it in units of standard deviation. For binary outcomes, odds ratios or relative risks should be considered.</p>
<p><b>Step 5:<i> </i></b><i>Calculate the overall effect by combining the data<b>. </b></i>In line with the previous step, it seems obvious from a methodological and statistical point of view that simple arithmetic averages are not a reliable way of comparing outcomes; for example, different sample sizes have different statistical power and, therefore, should be given different weights. Hence, a weighted average of the results should be used, in which the larger trials have more influence than the smaller ones. The statistical techniques to tackle this can be classified into two broad models: "fixed effects" model and "random effects" model. The choice between or the other will depend on the way the variability of the results between the studies we have chosen is treated. The first model tells us that random variation is the sole cause for this variability, meaning that the size of the studies is irrelevant to the type of results they give. The second, "random effects" model considers, and more reasonably, as sources of variation different underlying effects in each study, leading to larger confidence intervals than the fixed effects model.</p>
<p><b>That all sounds lovely, but what are the risks that a meta-analysis might carry?</b></p>
<p>Indeed, conducting a meta-analysis sounds like a very attractive approach to combining information into one concrete output that (hopefully) brings us clarity into a topic through a thorough and systematic review of previous research. However, as in any other statistical/methodological approach to research, biases are still there to bug us in the reliabiliy of meta-analyses. Egger &amp; Davey Smith (1998) present us with several biases that are most common in meta-analyses.</p>
<p><i>Publication bias</i><b>.</b> A study by Easterbrook et al. (1991) showed that the odds of publishing results were three times greater if the results were significant. Consequently, many authors have often avoided submitting their results for publishing simply due to the fact that they were not significant, making selective submission rather than selective acceptance a frequent issue.</p>
<p><i>Database bias</i>. Despite being a very good source of scholarly information, the most frequently accessed databases publish predominantly reports from developed counries and tend to neglect existing literature from less developed countries.</p>
<p><i>Citation bias</i>. In line with what was mentioned above in terms of having a good inclusion criteria of papers for analysis, often times the reference lists of  manuscripts will often include studies that are strongly supportive of the published results.</p>
<p><i>Multiple publication bias</i>. It is quite common for researchers to publish multiple manuscripts based on one single research conducted, and it is often very difficult (if not impossible) to distinguish if two papers are original researches or duplicates from the same trials.</p>
<p><i>Bias in provision of data</i>. Naturally, conducting a meta-analysis will strongly depend on the willingness of researchers to provide us with their data set for analysis, and this could prove to be a strong bias if researchers are not willing, possibly due to the direction of the results of their research.</p>
<p><i>Biased inclusion criteria</i>. As abovementioned, an author who is familiar with the topic he wishes to assess might be influenced towards selecting papers that might corroborate what they want to look for and ignore other sets of information that could be relevant but contradicting.</p>
<p><b>Overall, meta-analysis seems like an excellent systematic review tool, but is there any way around all this biasing, then?</b></p>
<p>Fighting bias in is one of the most challenging tasks in any research design. However, there are ways of trying to avoid it as much as possible to make your research as reliable as possible. Being aware of all of these confounding factors significantly reduces the lack of statistical reliability of your meta-analysis and will help you in structuring a more efficient strategy. Regarding the selection of the studies, it is important to include all studies that match the basic inclusion criteria, independently of any unexpeted predictions, followed by sensitivity analyses considering the different entry criteria (Egger &amp; Smith, 1998). Another method of tackling the presence or absence of bias is through funnel plots (i.e., simple scatterplots of the trials’ effect estimates against their sample size). (Egger, Davey Smith, Schneider, &amp; Minder, 1997).</p>
<p>&nbsp;</p>
<p><strong>References</strong></p>
<p>Easterbrook, P. J., Berlin, J. A., Gopalan, R., &amp; Matthews, D.R. (1991) Publication bias in clinical research. <i>Lancet</i>, <i>337</i>(8746), 867-872.</p>
<p>Egger, M. &amp; Davey Smith, G. (1997). Meta-analysis: Potentials and promise. <em>British Medical Journal, 315</em>(7119), 1371-1374.</p>
<p>Egger, M. &amp; Davey Smith, G. (1998). Bias in location and selection of studies. <i>British Medical Journal, 316</i>(7124), 61-66.</p>
<p>Egger, M., Davey Smith, G., &amp; Phillips, A. N. (1997). Meta-analysis: Principles and Procedures. <i>British Medical Journal, 315</i>(7121), 1533-1537.</p>
<p>Egger, M., Davey Smith, G., Schneider, M., &amp; Minder, C. E. (1997). Bias in meta-analysis detected by a simple, graphical test. <i>British Medical Journal, 315</i>(7129), 629-34</p>
<p>Freiman J. A., Chalmers T. C., Smith H., Kuebler R. R. (1992). The importance of beta, the type II error, and sample size in the design and interpretation of the randomized controlled trial. In Bailar, J. C., Mosteller, F. (Eds.), <i>Medical uses of statistics</i>. (357-373) Boston, MA: NEJM Books.</p>
<p>Huque, M. F. (1998). Experiences with meta-analysis in NDA submissions. <i>Proceedings of the Biopharmaceutical Section of the American Statistical Association. 2</i>, 28-33.</p>
<p>Moher, D., Jadad, A. R., Nichol, G., Penman, M., Tugwell, P., &amp; Walsh, S. (1995). Assessing the quality of randomized controlled trials: an annotated bibliography of scales and checklists. <i>Controlled Clinical Trials, 16</i>(1), 62-73.</p>
